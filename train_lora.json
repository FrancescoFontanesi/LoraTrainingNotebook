{"id":"a4b1f1d3-f016-40df-9141-9745337e53d6","revision":0,"last_node_id":23,"last_link_id":37,"nodes":[{"id":18,"type":"Note Plus (mtb)","pos":[411,-278],"size":[1447.654541015625,361],"flags":{},"order":0,"mode":0,"inputs":[],"outputs":[],"title":"Unnamed","properties":{"widget_ue_connectable":{}},"widgets_values":["\n## [Eden](https://www.eden.art/)\nThis LoRa trainer was made by the team behind https://www.eden.art/, led by https://x.com/xsteenbrugge\n\nIf you make awesome stuff w this trainer, give us a shout at:\nhttps://x.com/eden_art_ or\nhttps://www.instagram.com/eden.art____/\n\n---\n\n---\n\n## SD15 + SDXL:\nThis trainer works for both SD15 and SDXL models, but the default settings are primarily tuned for SDXL models. By selecting a specific ckpt_name, the trainer will automatically know if its an SDXL or SD15 model!\n\n---\n\n### A note on Embeddings:\nThis trainer optionally trains a textual inversion token into the LoRa, this is highly recommended when using SDXL models, but also means you have to load that token embedding when doing inference! See the example workflows in the repo.\n\n\nThe default settings work great for SDXL, SD15 usually need more training steps (eg 800) and sometimes benefits from disabling ti_training.\n\n---\n\n### A note on captioning:\nImages will get automatically captioned. It is recommended to put a .env file in the root of this custom node repo with your OpenAI API key, if found that will trigger a prompt_cleanup function that significantly improves results!\n","markdown","","one_dark"],"color":"#432","bgcolor":"#653","shape":1},{"id":19,"type":"Note Plus (mtb)","pos":[48,106],"size":[325.2762145996094,629],"flags":{},"order":1,"mode":0,"inputs":[],"outputs":[],"title":"Unnamed","properties":{"widget_ue_connectable":{}},"widgets_values":["\n## A note on settings:\n\nIf you disbale\\_ti (not recommended) you'll get a normal LoRa that does not use a token embedding, in that case you typically need to train for a bit longer and increase the unet_lr.\n\n---\n\n\"training_images\" can both be a path to a local folder or a url to a public .zip file of imgs which will get downloaded.\n\n---\n\nYou can provide custom captions by placing a filename.txt file for each filename.jpg in the training_images folder\n\n---\n\nI highly recommend to keep the training resolution at either 512 or 768.\n\n---\n\nn_tokens = 1 is currently broken, need to fix that.\n\n---\n\nAn embedding + LoRa checkpoint will get saved every **save\\_checkpoint\\_every\\_n\\_steps**. Based on the sample image grid you can then pick the best checkpoint to use in your workflows!\n\n---\n\nSetting debug=True will save a bunch of additional graphs and visualizations to track whats happening during training for advanced users.","markdown","","one_dark"],"color":"#432","bgcolor":"#653","shape":1},{"id":4,"type":"Display Any (rgthree)","pos":[909.4879760742188,358.7030029296875],"size":[349.7635803222656,88.69296264648438],"flags":{},"order":5,"mode":0,"inputs":[{"dir":3,"localized_name":"source","name":"source","type":"*","link":34}],"outputs":[],"properties":{"cnr_id":"rgthree-comfy","ver":"5dc53323e07a021038af9f2a4a06ebc071f7218c","Node name for S&R":"Display Any (rgthree)","widget_ue_connectable":{}},"widgets_values":[""]},{"id":3,"type":"Display Any (rgthree)","pos":[911.3204956054688,197.3616180419922],"size":[347.36749267578125,88],"flags":{},"order":4,"mode":0,"inputs":[{"dir":3,"localized_name":"source","name":"source","type":"*","link":33}],"outputs":[],"properties":{"cnr_id":"rgthree-comfy","ver":"5dc53323e07a021038af9f2a4a06ebc071f7218c","Node name for S&R":"Display Any (rgthree)","widget_ue_connectable":{}},"widgets_values":[""]},{"id":23,"type":"Display Any (rgthree)","pos":[1370.301025390625,204.2595977783203],"size":[347.36749267578125,88],"flags":{},"order":3,"mode":0,"inputs":[{"dir":3,"localized_name":"source","name":"source","type":"*","link":37}],"outputs":[],"properties":{"cnr_id":"rgthree-comfy","ver":"5dc53323e07a021038af9f2a4a06ebc071f7218c","widget_ue_connectable":{},"Node name for S&R":"Display Any (rgthree)"},"widgets_values":[""]},{"id":21,"type":"Eden_LoRa_trainer","pos":[413,124],"size":[412.5926513671875,591.4500122070312],"flags":{},"order":2,"mode":0,"inputs":[{"localized_name":"training_images_folder_path","name":"training_images_folder_path","type":"STRING","widget":{"name":"training_images_folder_path"},"link":null},{"localized_name":"mode","name":"mode","type":"COMBO","widget":{"name":"mode"},"link":null},{"localized_name":"lora_name","name":"lora_name","type":"STRING","widget":{"name":"lora_name"},"link":null},{"localized_name":"ckpt_name","name":"ckpt_name","type":"COMBO","widget":{"name":"ckpt_name"},"link":null},{"localized_name":"training_resolution","name":"training_resolution","type":"INT","widget":{"name":"training_resolution"},"link":null},{"localized_name":"train_batch_size","name":"train_batch_size","type":"INT","widget":{"name":"train_batch_size"},"link":null},{"localized_name":"max_train_steps","name":"max_train_steps","type":"INT","widget":{"name":"max_train_steps"},"link":null},{"localized_name":"ti_lr","name":"ti_lr","type":"FLOAT","widget":{"name":"ti_lr"},"link":null},{"localized_name":"unet_lr","name":"unet_lr","type":"FLOAT","widget":{"name":"unet_lr"},"link":null},{"localized_name":"lora_rank","name":"lora_rank","type":"INT","widget":{"name":"lora_rank"},"link":null},{"localized_name":"disable_ti","name":"disable_ti","type":"BOOLEAN","widget":{"name":"disable_ti"},"link":null},{"localized_name":"n_tokens","name":"n_tokens","type":"INT","widget":{"name":"n_tokens"},"link":null},{"localized_name":"save_checkpoint_every_n_steps","name":"save_checkpoint_every_n_steps","type":"INT","widget":{"name":"save_checkpoint_every_n_steps"},"link":null},{"localized_name":"n_sample_imgs","name":"n_sample_imgs","type":"INT","widget":{"name":"n_sample_imgs"},"link":null},{"localized_name":"sample_imgs_lora_scale","name":"sample_imgs_lora_scale","type":"FLOAT","widget":{"name":"sample_imgs_lora_scale"},"link":null},{"localized_name":"plot_training_graphs_on_disk","name":"plot_training_graphs_on_disk","type":"BOOLEAN","widget":{"name":"plot_training_graphs_on_disk"},"link":null},{"localized_name":"seed","name":"seed","type":"INT","widget":{"name":"seed"},"link":null}],"outputs":[{"localized_name":"lora_path","name":"lora_path","type":"STRING","slot_index":0,"links":[37]},{"localized_name":"embedding_path","name":"embedding_path","type":"STRING","slot_index":1,"links":[33]},{"localized_name":"final_msg","name":"final_msg","type":"STRING","slot_index":2,"links":[34]},{"localized_name":"final_msg","name":"final_msg","type":"STRING","slot_index":3,"links":[]}],"properties":{"cnr_id":"sd-lora-trainer","ver":"686a36126bc0f87fc2750d93f0c8ca1079909dd0","Node name for S&R":"Eden_LoRa_trainer","widget_ue_connectable":{}},"widgets_values":["D:\\Python Scripts\\AiArts\\lora\\resized_dataset","style","BrainRot","RealitiesEdgeXLLIGHTNING_TURBOV7.safetensors",512,1,100,0.001,0.0005,16,false,3,50,2,0.2,false,37724,"randomize"]}],"links":[[33,21,1,3,0,"*"],[34,21,2,4,0,"*"],[37,21,0,23,0,"*"]],"groups":[],"config":{},"extra":{"ds":{"scale":1.0152559799477652,"offset":[-152.44836246285786,39.95036711138883]},"ue_links":[],"links_added_by_ue":[],"frontendVersion":"1.17.11","VHS_latentpreview":false,"VHS_latentpreviewrate":0,"VHS_MetadataImage":true,"VHS_KeepIntermediate":true},"version":0.4}